{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underwater image enhancement model with PyTorch\n",
    "\n",
    "This notebook contains the `model archithecture` for `Genaretor`\n",
    "\n",
    "The generator is based on `ResNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import functools\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"Define a Resnet block\"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.BatchNorm2d, padding_type='reflect', use_dropout=True, use_bias=True):\n",
    "        \"\"\"Initialize the Resnet block\n",
    "\n",
    "        Construct a convolutional block.\n",
    "\n",
    "        :param dim: the number of channels in the conv layer.\n",
    "        :param norm_layer: normalization layer\n",
    "        :param padding_type: the name of padding layer: reflect | replicate | zero\n",
    "        :param use_dropout: if use dropout layers.\n",
    "        :param use_bias: if the conv layer uses bias or not\n",
    "\n",
    "        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n",
    "\n",
    "\n",
    "        A resnet block is a conv block with skip connections\n",
    "        We construct a conv block with build_conv_block function,\n",
    "        and implement skip connections in <forward> function.\n",
    "        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "        \"\"\"\n",
    "        \n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, norm_layer, padding_type, use_dropout, use_bias)\n",
    "        \n",
    "        \n",
    "    def build_conv_block(self, dim, norm_layer, padding_type, use_dropout, use_bias):\n",
    "        \"\"\"Initialize the Resnet block\n",
    "\n",
    "        Construct a convolutional block.\n",
    "\n",
    "        :param dim: the number of channels in the conv layer.\n",
    "        :param norm_layer: normalization layer\n",
    "        :param padding_type: the name of padding layer: reflect | replicate | zero\n",
    "        :param use_dropout: if use dropout layers.\n",
    "        :param use_bias: if the conv layer uses bias or not\n",
    "        :return: conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n",
    "        \"\"\"\n",
    "        conv_block = []\n",
    "\n",
    "        # First Reflection Padding\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        # First conv layer\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
    "                       norm_layer(dim),\n",
    "                       nn.ReLU(True)]\n",
    "\n",
    "        # First dropout layer\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        # First Reflection Padding\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
    "                       norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function (with skip connections)\"\"\"\n",
    "        \n",
    "        out = x + self.conv_block(x)  # add skip connections\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResnetBlock(\n",
       "  (conv_block): Sequential(\n",
       "    (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (1): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "    (6): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (7): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResnetBlock(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetGenerator(nn.Module):\n",
    "    \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n",
    "\n",
    "    We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, padding_type='reflect', use_dropout=False, n_blocks=9):\n",
    "        \"\"\"Construct a Resnet-based generator\n",
    "\n",
    "        :param input_nc: the number of channels in input images\n",
    "        :param output_nc: the number of channels in output images\n",
    "        :param ngf: the number of filters in the last conv layer\n",
    "        :param norm_layer: normalization layer\n",
    "        :param padding_type: the name of padding layer: reflect | replicate | zero\n",
    "        :param use_dropout: if use dropout layers\n",
    "        :param n_blocks: the number of ResNet blocks\n",
    "        \"\"\"\n",
    "        \n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        \n",
    "        # use_bias is set false if BatchNorm2d is used as norm layer\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "            \n",
    "        # First Conv layer\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        # 2 Down sampling layers (2nd and 3rd)\n",
    "        n_down_sampling = 2\n",
    "        for i in range(n_down_sampling):\n",
    "            mult = 2 ** i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2 ** n_down_sampling\n",
    "        \n",
    "        # n_blocks resnet layers\n",
    "        for i in range(n_blocks):\n",
    "\n",
    "            model += [ResnetBlock(ngf * mult, norm_layer=norm_layer, padding_type=padding_type, use_dropout=use_dropout, use_bias=use_bias)]\n",
    "\n",
    "        # Add up sampling Layers\n",
    "        for i in range(n_down_sampling):\n",
    "            mult = 2 ** (n_down_sampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResnetGenerator(\n",
       "  (model): Sequential(\n",
       "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): ResnetBlock(\n",
       "      (conv_block): Sequential(\n",
       "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): ReLU(inplace=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
       "        (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "        (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (19): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "    (20): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (21): ReLU(inplace=True)\n",
       "    (22): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
       "    (23): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): ReflectionPad2d((3, 3, 3, 3))\n",
       "    (26): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (27): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResnetGenerator(3,3, use_dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 256, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResnetGenerator(3,3, use_dropout=True)\n",
    "res= model(torch.autograd.Variable(torch.Tensor(1,3,256,256), requires_grad=True))\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "writer.add_graph(model, torch.autograd.Variable(torch.Tensor(1,3,256,256), requires_grad=True))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}