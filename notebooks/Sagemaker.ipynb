{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0e2086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/uwie'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "793dbe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-east-1-641697043155/sagemaker/uwie\n"
     ]
    }
   ],
   "source": [
    "inputs = sagemaker_session.upload_data(path='Dataset', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4689fe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdata\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m create_dataset\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m create_model\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36moptions\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mTrainOptions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainOptions\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mTrainStats\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TrainStats\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msetup_cloud\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m setup_cloud\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    opt = TrainOptions().parse()\n",
      "    opt = setup_cloud(opt)\n",
      "\n",
      "    dataset = create_dataset(dataroot=opt.training_data_dir, subdir=opt.subdir, phase=opt.phase,\n",
      "                             serial_batches=opt.serial_batches, preprocess=opt.preprocess, no_flip=opt.no_flip,\n",
      "                             load_size=opt.load_size, crop_size=opt.crop_size, batch_size=opt.batch_size,\n",
      "                             is_distributed=opt.is_distributed, use_cuda=opt.use_cuda)\n",
      "\n",
      "    dataset_size = \u001b[36mlen\u001b[39;49;00m(dataset)\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mThe number of training images = \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % dataset_size)\n",
      "\n",
      "    model = create_model(opt)\n",
      "    stats = TrainStats(opt)\n",
      "    \n",
      "\n",
      "    \u001b[37m# Training\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + \u001b[34m1\u001b[39;49;00m):\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mTraining \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mopt.n_epochs + opt.n_epochs_decay + \u001b[34m1\u001b[39;49;00m\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[37m# Training\u001b[39;49;00m\n",
      "        epoch_start_time = time.time()\n",
      "        model.train()\n",
      "        \u001b[34mfor\u001b[39;49;00m i, data \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(dataset):\n",
      "            model.feed_input(data)\n",
      "            model.optimize_parameters()\n",
      "\n",
      "        training_end_time = time.time()\n",
      "        \u001b[37m# Training block ends\u001b[39;49;00m\n",
      "\n",
      "        \u001b[37m# Evaluation\u001b[39;49;00m\n",
      "        model.eval()\n",
      "        t_data = training_end_time - epoch_start_time  \u001b[37m# Training Time\u001b[39;49;00m\n",
      "        t_comp = t_data / opt.batch_size  \u001b[37m# Single input time\u001b[39;49;00m\n",
      "\n",
      "        \u001b[37m# Save model generated images and losses\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m epoch % opt.visuals_freq == \u001b[34m0\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving Visuals (epoch: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            stats.save_current_visuals(model.get_current_visuals(), \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mimg-\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            stats.print_current_losses(epoch, model.get_current_losses(), t_comp, t_data)\n",
      "\n",
      "        \u001b[37m# Save model artifacts\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m epoch % opt.artifact_freq == \u001b[34m0\u001b[39;49;00m:\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33msaving the model at the end of epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            model.save_networks(\u001b[36mstr\u001b[39;49;00m(epoch))\n",
      "            model.save_optimizers_and_scheduler(\u001b[36mstr\u001b[39;49;00m(epoch))\n",
      "        \u001b[37m# Evaluation block ends\u001b[39;49;00m\n",
      "\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mEnd of epoch \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mepoch\u001b[33m}\u001b[39;49;00m\u001b[33m / \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mopt.n_epochs + opt.n_epochs_decay\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "              \u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mTime Taken: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtime.time() - epoch_start_time\u001b[33m}\u001b[39;49;00m\u001b[33m sec\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        model.update_learning_rate()\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnd of training!!!\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab2b9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='train.py',\n",
    "                    role=role,\n",
    "                    source_dir='.',\n",
    "                    py_version='py3',\n",
    "                    framework_version='1.8.1',\n",
    "                    instance_count=2,\n",
    "                    instance_type='ml.g4dn.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'model':'cyclegan2',\n",
    "                        'preprocess': 'RRC',\n",
    "                        'n_epochs': 20,\n",
    "                        'n_epochs_decay': 0,\n",
    "                        'backend': 'nccl',\n",
    "                        'batch-size': 8\n",
    "                    },metric_definitions=[\n",
    "                        {'Name': 'loss:D_A', 'Regex': 'loss_D_A=(.*?);'},\n",
    "                        {'Name': 'loss:D_B', 'Regex': 'loss_D_B=(.*?);'},\n",
    "                        {'Name': 'loss:G_AtoB', 'Regex': 'loss_G_AtoB=(.*?);'},\n",
    "                        {'Name': 'loss:G_BtoA', 'Regex': 'loss_G_BtoA=(.*?);'},\n",
    "                        {'Name': 'cycle_loss:A', 'Regex': 'cycle_loss_A=(.*?);'},\n",
    "                        {'Name': 'cycle_loss:B', 'Regex': 'cycle_loss_B=(.*?);'},\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8587e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit({'training': 's3://sagemaker-us-east-1-641697043155/sagemaker/uwie'},wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e35f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
